{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgkePud9Yc29fy7kqQZOQq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sogand120/sogandsa.github.io/blob/master/sample_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr5CUlRO7UNF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AlbertTokenizer, AlbertForSequenceClassification, AdamW\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "## BDD related keywords (not yet comprehensive, many more to add)\n",
        "bdd_keywords = [\"body image\", \"mirror\", \"hate\", \"BDD\", \"diagnosed with BDD\", \"flaw\", \"acne\", \"dislike\", \"body hatred\", \"body comparison\", \"fat\", \"overweight\",\n",
        "                \"scar\", \"gained weight\", \"hate my body\", \"too fat\", \"feel ugly\" ]\n",
        "\n",
        "## parse XML function\n",
        "def parse_xml(file_path):\n",
        "    data = []\n",
        "    try:\n",
        "        tree = ET.parse(file_path)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        for element in root.findall('.//record'):  # Adjust based on your XML structure\n",
        "            text = element.find('text').text\n",
        "            label = element.find('label').text\n",
        "            data.append({'text': text, 'label': label})\n",
        "    except ET.ParseError as e:\n",
        "        logger.error(f\"Error parsing {file_path}: {e}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An unexpected error occurred with {file_path}: {e}\")\n",
        "    return data\n",
        "\n",
        "# custom Dataset to handle XML files and tokenization\n",
        "class BDDExtendedDataset(Dataset):\n",
        "    def __init__(self, xml_files, tokenizer, max_len):\n",
        "        self.data = []\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "        ## parse XML files\n",
        "        for file in xml_files:\n",
        "            self.data.extend(parse_xml(file))\n",
        "\n",
        "        logger.info(f\"Loaded {len(self.data)} records from XML files.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]['text']\n",
        "        label = self.data[idx]['label']\n",
        "\n",
        "        ## tokenize\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        label = torch.tensor(int(label), dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': label,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "## keyword detection function\n",
        "def detect_keywords(texts, keywords):\n",
        "    keyword_set = set(keywords)\n",
        "    results = []\n",
        "    for text in texts:\n",
        "        words = set(text.lower().split())\n",
        "        if keyword_set.intersection(words):\n",
        "            results.append(True)\n",
        "        else:\n",
        "            results.append(False)\n",
        "    return results\n",
        "\n",
        "## main function\n",
        "def main():\n",
        "    xml_folder = '/path/to/xml_folder'  ## adjust according to folder\n",
        "    xml_files = [os.path.join(xml_folder, file) for file in os.listdir(xml_folder) if file.endswith('.xml')]\n",
        "\n",
        "    tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "    model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)\n",
        "\n",
        "    dataset = BDDExtendedDataset(xml_files, tokenizer, max_len=128)\n",
        "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "    ## training Loop\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(3):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in tqdm(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        logger.info(f'Epoch {epoch + 1}, Training Loss: {train_loss / len(dataloader)}')\n",
        "\n",
        "    logger.info(\"Performing keyword detection...\")\n",
        "\n",
        "    texts = [item['text'] for item in dataset]\n",
        "    keyword_results = detect_keywords(texts, bdd_keywords)\n",
        "\n",
        "    ## output\n",
        "    for i, result in enumerate(keyword_results):\n",
        "        logger.info(f\"Text: {texts[i][:50]}...\")  # Print the first 50 characters of each text\n",
        "        logger.info(f\"BDD Keywords Found: {result}\")\n",
        "\n",
        "print(main())"
      ]
    }
  ]
}